{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4114, 0.1927, 0.1980, 0.1979],\n",
      "         [0.3803, 0.3329, 0.1926, 0.0943],\n",
      "         [0.3818, 0.3147, 0.1800, 0.1235]],\n",
      "\n",
      "        [[0.4161, 0.1665, 0.2039, 0.2135],\n",
      "         [0.2720, 0.2323, 0.1895, 0.3062],\n",
      "         [0.3177, 0.2436, 0.1880, 0.2507]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 2, 3]) tensor([4, 5, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "from torch.nn.init import constant_, xavier_uniform_\n",
    "torch.manual_seed(4)\n",
    "\n",
    "class PositionalEncoding(nn.Module):  # 位置编码，用在输入层，每一层的开始，编码器，解码器开始\n",
    "\n",
    "    def __init__(self, d_model, n_position=200, device=None, dtype=None):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        # d_hid是维度，即每个单词由长度为多少的向量进行表示。\n",
    "        # n_position为最大位置，即所能处理的单词在句子中的最大的位置\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_position = n_position # 其实应该就是maxlen\n",
    "        # 将tensor注册成buffer, optim.step()的时候不会更新\n",
    "        # 定义一组参数，参数名称为‘pos_table’，\n",
    "        # 这组tensor形式的参数为self._get_sinusoid_encoding_table(n_position, d_hid)\n",
    "        self.register_buffer('pos_table', self._get_sinusoid_encoding_table(n_position, d_model))\n",
    "\n",
    "    def _get_sinusoid_encoding_table(self, n_position, d_model):\n",
    "        # TODO: make it with torch instead of numpy   todo是将要做的事，用在团队协作中，好处是和注释分开\n",
    "        def get_position_angle_vec(pos):\n",
    "            # 2 * (i // 2) 依次为：0 0 2 2 4 4 6 6 8 8 ... 然后/d_hid归一化到0~1，再变到指数\n",
    "            # 变到指数范围在1~10000,然后用pos除以这个指数，得到范围0~200/10000，最后偶维度用sin 奇维度用cos\n",
    "            return [pos / torch.pow(torch.tensor(10000), torch.tensor(2 * (i // 2) / d_model)) for i in range(d_model)]\n",
    "\n",
    "        # 创建一个位置的array，每一行是位置i的位置嵌入向量\n",
    "        # 列表列数为词嵌入维度，列表行数为单词个数\n",
    "        sinusoid_table = torch.tensor([get_position_angle_vec(pos) for pos in range(n_position)]) # 二维的一个表格\n",
    "\n",
    "        # 返回每个位置的sin/cos括号里面的\n",
    "        sinusoid_table[:, 0::2] = torch.sin(sinusoid_table[:, 0::2])  # dim 2i 偶数 奇数偶数嵌入维度分别取sin/cos值\n",
    "        sinusoid_table[:, 1::2] = torch.cos(sinusoid_table[:, 1::2])  # dim 2i+1 奇数\n",
    "        # 转为Float类型的张量  # unsqueeze(0)是升维，在0维度升一维，最高维是batchsize所在的维度\n",
    "        return sinusoid_table\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:  # [[1,2,3], [4,5,6]] 0维是2，1维是3，越高就是越靠里面的\n",
    "        # clone()创建一个相同的tensor，不共享内存地址（数据无关），但是新tensor的梯度会叠加在源tensor上\n",
    "        # detach()是返回一个共享内存（数值一样），但是不计算grad的tensor\n",
    "        # x应该是已经经过词编码的向量 [:, :词嵌入维度d_hid]\n",
    "        return x + self.pos_table.clone().detach()  # 数据、梯度均无关，索引编码+位置编码\n",
    "    \n",
    "def create_mask(src_seq: Tensor, tgt_seq: Tensor, PAD_IDX):\n",
    "    is_batched = src_seq.dim() == 2\n",
    "    if is_batched:\n",
    "        src_seq_len = src_seq.shape[1]\n",
    "        tgt_seq_len = tgt_seq.shape[1]\n",
    "    else:\n",
    "        src_seq_len = src_seq.shape[0]\n",
    "        tgt_seq_len = tgt_seq.shape[0]\n",
    "\n",
    "    tgt_mask = (torch.triu(torch.ones(tgt_seq_len, tgt_seq_len)) == 1).transpose(0, 1)\n",
    "    tgt_mask = tgt_mask.float().masked_fill(tgt_mask == 0, float('-inf')).masked_fill(tgt_mask == 1, float(0.0))\n",
    "    src_mask = torch.zeros( (src_seq_len, src_seq_len) ).type(torch.bool) # 一般源不需要mask，所以都置为0\n",
    "\n",
    "    src_key_padding_mask = (src_seq == PAD_IDX) # 后面的都会变成1作为mask\n",
    "    tgt_key_padding_mask = (tgt_seq == PAD_IDX)\n",
    "    return src_mask, tgt_mask, src_key_padding_mask, tgt_key_padding_mask\n",
    "\n",
    "\n",
    "src_seq = torch.LongTensor([[1,2,0],[1,3,0]])\n",
    "tgt_seq = torch.LongTensor([[1,3,0],[2,1,0]])\n",
    "num_heads = 4\n",
    "embed_dim = 4\n",
    "\n",
    "src_mask, tgt_mask, src_key_padding_mask, tgt_key_padding_mask = create_mask(src_seq, tgt_seq, 0)\n",
    "# print(src_mask)\n",
    "# print(tgt_mask)\n",
    "# print(src_key_padding_mask)\n",
    "# print(tgt_key_padding_mask)\n",
    "\n",
    "# embedding = nn.Embedding(num_embeddings=embed_dim, embedding_dim=embed_dim,padding_idx=0)\n",
    "src_embedding = nn.Embedding(num_embeddings=embed_dim, embedding_dim=embed_dim,padding_idx=0)\n",
    "tgt_embedding = nn.Embedding(num_embeddings=embed_dim, embedding_dim=embed_dim,padding_idx=0)\n",
    "pose = PositionalEncoding(d_model=embed_dim,n_position=3)\n",
    "dropout = nn.Dropout(0.1)\n",
    "layer_norm = nn.LayerNorm(4, eps=0.00001)\n",
    "Model_nnTransformer = nn.Transformer(d_model=embed_dim, nhead=num_heads, batch_first=True)\n",
    "tgt_out = nn.Linear(4, 4, bias=False)\n",
    "src_input = layer_norm(dropout(pose(src_embedding(src_seq))))\n",
    "tgt_input = layer_norm(dropout(pose(tgt_embedding(tgt_seq))))\n",
    "output = Model_nnTransformer(src_input, tgt_input, src_mask=src_mask, tgt_mask=tgt_mask,\n",
    "                memory_mask=None, src_key_padding_mask=src_key_padding_mask,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=None)\n",
    "\n",
    "output = F.softmax(tgt_out(output),-1)\n",
    "print(output)\n",
    "\n",
    "# x1 = torch.tensor([\n",
    "#     [ [[1,2,1],[1,2,1],[1,2,1]], [[1,2,1],[1,2,1],[1,2,1]], [[1,2,1],[1,2,1],[1,2,1]], [[1,2,1],[1,2,1],[1,2,1]] ],\n",
    "#     [ [[1,2,1],[1,2,1],[1,2,1]], [[1,2,1],[1,2,1],[1,2,1]], [[1,2,1],[1,2,1],[1,2,1]], [[1,2,1],[1,2,1],[1,2,1]] ] ])\n",
    "# y1 = torch.tensor([[[[0,0,-99999]]],[[[0,-99999,-99999]]]])\n",
    "# print(x1.shape)\n",
    "# print(y1.shape)\n",
    "# print(x1)\n",
    "# print(y1)\n",
    "# print(x1+y1)\n",
    "# tgt_mask = (torch.triu(torch.ones(6, 6)) == 1).transpose(0, 1)\n",
    "# print(tgt_mask)\n",
    "\n",
    "i_seq = torch.linspace(0, 10 - 1, 10)\n",
    "j_seq = torch.linspace(0, 16 - 2, 16 // 2)\n",
    "pos, two_i = torch.meshgrid(i_seq, j_seq)\n",
    "# print(pos.shape)\n",
    "# # print(pos)\n",
    "# print(two_i)\n",
    "x = torch.tensor([[1,2,3],[4,5,7]])\n",
    "print(*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "tensor([[[ 0.0476,  0.3066, -0.8684,  0.5803],\n",
      "         [ 0.1157,  0.4071, -0.5418,  0.4576],\n",
      "         [ 0.0593,  0.3850, -0.6665,  0.4705]],\n",
      "\n",
      "        [[-0.3918,  0.8157, -0.9510,  0.1099],\n",
      "         [-0.2867,  0.9686, -0.8271,  0.2097],\n",
      "         [-0.3277,  0.6723, -1.0601,  0.2519]]], grad_fn=<TransposeBackward0>)\n",
      "tensor([[[[0.7595, 0.2405, 0.0000],\n",
      "          [0.3648, 0.6352, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000]],\n",
      "\n",
      "         [[0.6013, 0.3987, 0.0000],\n",
      "          [0.3939, 0.6061, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000]],\n",
      "\n",
      "         [[0.4774, 0.5226, 0.0000],\n",
      "          [0.5598, 0.4402, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000]],\n",
      "\n",
      "         [[0.5029, 0.4971, 0.0000],\n",
      "          [0.5396, 0.4604, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2195, 0.7805, 0.0000],\n",
      "          [0.1252, 0.8748, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000]],\n",
      "\n",
      "         [[0.4872, 0.5128, 0.0000],\n",
      "          [0.4765, 0.5235, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000]],\n",
      "\n",
      "         [[0.5182, 0.4818, 0.0000],\n",
      "          [0.5970, 0.4030, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000]],\n",
      "\n",
      "         [[0.4973, 0.5027, 0.0000],\n",
      "          [0.2786, 0.7214, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000]]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(4)\n",
    "batch_size = src_seq.shape[0]\n",
    "seq_len = src_seq.shape[1]\n",
    "embedding = nn.Embedding(num_embeddings=embed_dim, embedding_dim=embed_dim,padding_idx=0)\n",
    "query = embedding(src_seq) # N L E batchsize seq_length embedding_dim\n",
    "query = key = value = query.transpose(1, 0) # 输入的qkv L N E\n",
    "in_proj_weight = nn.Parameter(torch.empty((seq_len * embed_dim, embed_dim))) # 权重Wq Wk Wv\n",
    "xavier_uniform_(in_proj_weight)\n",
    "in_proj_bias = nn.Parameter(torch.empty(seq_len * embed_dim,)) # 偏置\n",
    "out_proj = nn.Linear(embed_dim, embed_dim, bias=True) \n",
    "constant_(in_proj_bias, 0.0) # 使用指定的值填充tensor\n",
    "constant_(out_proj.bias, 0.0)\n",
    "# 将屏蔽转换为标准格式 并确保其他类型的屏蔽（如果存在）与目标屏蔽具有相同的数据类型\n",
    "src_key_padding_mask = F._canonical_mask( # N S batchsize source_seq_length\n",
    "    mask=src_key_padding_mask,\n",
    "    mask_name=\"key_padding_mask\",\n",
    "    other_type=F._none_or_dtype(src_mask), # 如果存在就返回其数据类型，否则返回None\n",
    "    other_name=\"attn_mask\",\n",
    "    target_type=query.dtype # 用于指定屏蔽的数据类型\n",
    ")\n",
    "\n",
    "attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
    "            query, key, value, embed_dim_to_check = embed_dim, num_heads = num_heads,\n",
    "            in_proj_weight = in_proj_weight, in_proj_bias = in_proj_bias,\n",
    "            bias_k = None, bias_v= None, add_zero_attn = False,\n",
    "            dropout_p = 0.1, out_proj_weight = out_proj.weight, out_proj_bias = out_proj.bias,\n",
    "            training=False,\n",
    "            key_padding_mask=src_key_padding_mask, need_weights=True,\n",
    "            attn_mask=src_mask,\n",
    "            average_attn_weights=False,\n",
    "            is_causal=False)\n",
    "attn_output = attn_output.transpose(1, 0)\n",
    "print(attn_output.shape)\n",
    "print(attn_output)\n",
    "print(attn_output_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 4])\n",
      "tensor([[[ 0.0476,  0.3066, -0.8684,  0.5803],\n",
      "         [-0.3918,  0.8157, -0.9510,  0.1099]],\n",
      "\n",
      "        [[ 0.1157,  0.4071, -0.5418,  0.4576],\n",
      "         [-0.2867,  0.9686, -0.8271,  0.2097]],\n",
      "\n",
      "        [[ 0.0593,  0.3850, -0.6665,  0.4705],\n",
      "         [-0.3277,  0.6723, -1.0601,  0.2519]]], grad_fn=<TransposeBackward0>)\n",
      "tensor([[[[0.7595, 0.2405, 0.0000],\n",
      "          [0.3648, 0.6352, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000]],\n",
      "\n",
      "         [[0.6013, 0.3987, 0.0000],\n",
      "          [0.3939, 0.6061, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000]],\n",
      "\n",
      "         [[0.4774, 0.5226, 0.0000],\n",
      "          [0.5598, 0.4402, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000]],\n",
      "\n",
      "         [[0.5029, 0.4971, 0.0000],\n",
      "          [0.5396, 0.4604, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2195, 0.7805, 0.0000],\n",
      "          [0.1252, 0.8748, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000]],\n",
      "\n",
      "         [[0.4872, 0.5128, 0.0000],\n",
      "          [0.4765, 0.5235, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000]],\n",
      "\n",
      "         [[0.5182, 0.4818, 0.0000],\n",
      "          [0.5970, 0.4030, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000]],\n",
      "\n",
      "         [[0.4973, 0.5027, 0.0000],\n",
      "          [0.2786, 0.7214, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "key_padding_mask = src_key_padding_mask\n",
    "attn_mask = src_mask\n",
    "training = False\n",
    "dropout = nn.Dropout(0.1)\n",
    "\n",
    "Q = (torch.matmul(query,in_proj_weight[:embed_dim,:].transpose(0,1)) + in_proj_bias[:embed_dim]).transpose(0,1)\n",
    "K = (torch.matmul(key,in_proj_weight[embed_dim:2*embed_dim,:].transpose(0,1)) + in_proj_bias[embed_dim:2*embed_dim]).transpose(0,1)\n",
    "V = (torch.matmul(value,in_proj_weight[2*embed_dim:3*embed_dim,:].transpose(0,1)) + in_proj_bias[2*embed_dim:3*embed_dim]).transpose(0,1)\n",
    "Q = Q.view(batch_size, seq_len, embed_dim, (embed_dim//num_heads)).transpose(1,2) # batch_size seq_len num_heads head_dim\n",
    "K = K.view(batch_size, seq_len, embed_dim, (embed_dim//num_heads)).transpose(1,2) # batch_size num_heads seq_len head_dim\n",
    "V = V.view(batch_size, seq_len, embed_dim, (embed_dim//num_heads)).transpose(1,2)\n",
    "\n",
    "attn_output_weights = torch.matmul(Q / ((embed_dim//num_heads) ** 0.5 ), K.transpose(-2, -1))  # [batch_size, num_heads, seq_len, seq_len]\n",
    "if key_padding_mask is not None:\n",
    "    src_key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]\n",
    "    attn_output_weights = attn_output_weights.masked_fill(src_key_padding_mask !=0 , float('-inf'))  # Apply mask by setting scores to -inf\n",
    "else:\n",
    "    pass\n",
    "if attn_mask is not None:\n",
    "    attn_output_weights = attn_output_weights + attn_mask.unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "attn_output_weights = torch.softmax(attn_output_weights, dim=-1)\n",
    "if training:\n",
    "    attn_output_weights = dropout(attn_output_weights)\n",
    "# Apply attention weights to value\n",
    "attn_output = torch.matmul(attn_output_weights, V)  # [batch_size, num_heads, seq_len, embed_dim // num_heads]\n",
    "# # Transpose and reshape to restore original shape\n",
    "attn_output = attn_output.transpose(1, 2)  # [batch_size, seq_len, num_heads, embed_dim // num_heads]\n",
    "attn_output = attn_output.contiguous().view(batch_size, seq_len,embed_dim)  # [batch_size, seq_len, embed_dim]\n",
    "# # Apply output projection\n",
    "attn_output = out_proj(attn_output).transpose(0,1)\n",
    "print(attn_output.shape)\n",
    "print(attn_output)\n",
    "print(attn_output_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4300, -0.7901, -1.8083, -1.1338,  1.2966, -0.1208, -0.5116, -1.5176],\n",
      "        [ 0.4556, -1.6697,  0.0218, -0.8667,  1.3035,  0.6310, -0.9778,  0.3571],\n",
      "        [-0.8921,  1.6783,  1.1638,  0.5075, -1.1120,  1.3915, -0.5504,  0.1138],\n",
      "        [-0.8925, -2.2975, -0.7189,  0.6959, -0.3114,  0.1659,  1.3819, -1.3395],\n",
      "        [-0.6346, -0.6993, -1.3065,  0.5795,  0.4003, -0.3327, -0.6629,  0.1936],\n",
      "        [-0.6293, -0.7184,  1.1213, -0.1945, -2.0126, -0.2996, -1.1608,  0.0406],\n",
      "        [ 1.1254,  3.1009, -0.5068, -1.3209,  0.2486, -1.5305,  1.2565, -0.6270],\n",
      "        [ 0.5675, -0.2091, -1.6583, -0.8675,  0.7634, -0.6300, -0.7686, -0.3769],\n",
      "        [ 0.8233, -2.0816, -0.5910, -0.2266,  1.2819, -0.2478,  0.7555,  1.1254],\n",
      "        [-0.7683, -0.8072, -0.4347, -0.8952,  0.3564, -0.3828, -1.7014,  1.0922]],\n",
      "       requires_grad=True)\n",
      "tensor([[ 0.4300, -0.7901, -1.8083, -1.1338,  1.2966, -0.1208, -0.5116, -1.5176],\n",
      "        [ 0.4556, -1.6697,  0.0218, -0.8667,  1.3035,  0.6310, -0.9778,  0.3571],\n",
      "        [-0.8921,  1.6783,  1.1638,  0.5075, -1.1120,  1.3915, -0.5504,  0.1138],\n",
      "        [-0.8925, -2.2975, -0.7189,  0.6959, -0.3114,  0.1659,  1.3819, -1.3395],\n",
      "        [-0.6346, -0.6993, -1.3065,  0.5795,  0.4003, -0.3327, -0.6629,  0.1936],\n",
      "        [-0.6293, -0.7184,  1.1213, -0.1945, -2.0126, -0.2996, -1.1608,  0.0406],\n",
      "        [ 1.1254,  3.1009, -0.5068, -1.3209,  0.2486, -1.5305,  1.2565, -0.6270],\n",
      "        [ 0.5675, -0.2091, -1.6583, -0.8675,  0.7634, -0.6300, -0.7686, -0.3769],\n",
      "        [ 0.8233, -2.0816, -0.5910, -0.2266,  1.2819, -0.2478,  0.7555,  1.1254],\n",
      "        [-0.7683, -0.8072, -0.4347, -0.8952,  0.3564, -0.3828, -1.7014,  1.0922]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = nn.Embedding(10,8)\n",
    "print(a.weight)\n",
    "print(a(torch.tensor([0,1,2,3,4,5,6,7,8,9])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# x = torch.tensor([[1,3],[3,3],[0,2]])\n",
    "\n",
    "# print(torch.unsqueeze(x, -3))  # tensor([[1., 2., 3., 4.]])\n",
    "# print(torch.unsqueeze(x, -3).size())  # torch.Size([1, 4])\n",
    "# print(torch.unsqueeze(x, -3).dim())\n",
    "# # 0 -> 1 3 2\n",
    "# # 1 -> 3 1 2\n",
    "# # 2 -> 3 2 1\n",
    "# # -1 -> 3 2 1\n",
    "# # -2 -> 3 1 2\n",
    "# # -3 -> 1 3 2\n",
    "# print(torch.tensor([[1,2,3,4,5]]).size())\n",
    "# x1 = torch.tensor([1,3,4,6,7,98.5,4,23],requires_grad=True)\n",
    "# x2 = torch.tensor([98.5,1,3,4,6,7,4,23],requires_grad=True)\n",
    "# y = max((x1**2) * x2)\n",
    "# print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = query.shape[1] # [seq_len, batchsize, d_model]\n",
    "#         seq_len_q = query.shape[0] #50\n",
    "#         seq_len_k = key.shape[0]\n",
    "#         seq_len_v = value.shape[0]\n",
    "#         Q = (torch.matmul(query,self.in_QKV_weight[:self.embed_dim,:].transpose(0,1)) + self.in_QKV_bias[:self.embed_dim]).transpose(0,1)\n",
    "#         K = (torch.matmul(key,self.in_QKV_weight[self.embed_dim:2*self.embed_dim,:].transpose(0,1)) + self.in_QKV_bias[self.embed_dim:2*self.embed_dim]).transpose(0,1)\n",
    "#         V = (torch.matmul(value,self.in_QKV_weight[2*self.embed_dim:3*self.embed_dim,:].transpose(0,1)) + self.in_QKV_bias[2*self.embed_dim:3*self.embed_dim]).transpose(0,1)\n",
    "#         Q = Q.view(batch_size, seq_len_q, self.num_heads, (self.embed_dim//self.num_heads)).transpose(1,2) # batch_size seq_len num_heads head_dim\n",
    "#         K = K.view(batch_size, seq_len_k, self.num_heads, (self.embed_dim//self.num_heads)).transpose(1,2) # batch_size num_heads seq_len head_dim\n",
    "#         V = V.view(batch_size, seq_len_v, self.num_heads, (self.embed_dim//self.num_heads)).transpose(1,2)\n",
    "#         attn_output_weights = torch.matmul(Q / ((self.embed_dim) ** 0.5 ), K.transpose(-2, -1)).to(self.device)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "#         if key_padding_mask is not None:\n",
    "#             src_key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]\n",
    "#             # attn_output_weights = attn_output_weights.masked_fill(src_key_padding_mask !=0 , float('-inf'))  # Apply mask by setting scores to -inf\n",
    "#             attn_output_weights = attn_output_weights + src_key_padding_mask # Apply mask by setting scores to -inf\n",
    "#         else:\n",
    "#             pass\n",
    "#         if attn_mask is not None:\n",
    "#             attn_output_weights = attn_output_weights + attn_mask.unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "#         attn_output_weights = F.softmax(attn_output_weights, dim=-1)\n",
    "#         if self.training:\n",
    "#             attn_output_weights = self.dropout(attn_output_weights)\n",
    "#         # Apply attention weights to value\n",
    "#         attn_output = torch.matmul(attn_output_weights, V)  # [batch_size, num_heads, seq_len, embed_dim // num_heads]\n",
    "#         # # Transpose and reshape to restore original shape\n",
    "#         attn_output = attn_output.transpose(1, 2)  # [batch_size, seq_len, num_heads, embed_dim // num_heads]\n",
    "#         attn_output = attn_output.contiguous().view(batch_size, seq_len_q, self.embed_dim)  # [batch_size, seq_len, embed_dim] ##########################\n",
    "#         # # Apply output projection\n",
    "#         # attn_output = self.out_proj(attn_output).transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):  # 多头自注意力模块\n",
    "    ''' Multi-Head Attention module '''\n",
    "\n",
    "    def __init__(self, nhead, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nhead = nhead  # 多头，头的个数\n",
    "        self.d_k = d_k  # 单词向量的维度\n",
    "        self.d_v = d_v\n",
    "        # 三个线性层，创建方法，每个包含一个权重(d_model, nhead * d_k)为权重矩阵维度\n",
    "        self.w_qs = nn.Linear(d_model, nhead * d_k, bias=False)\n",
    "        self.w_ks = nn.Linear(d_model, nhead * d_k, bias=False)\n",
    "        self.w_vs = nn.Linear(d_model, nhead * d_v, bias=False)\n",
    "        self.fc = nn.Linear(nhead * d_v, d_model, bias=False)\n",
    "        # ScaledDotProductAttention见下方\n",
    "        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):  # q, k, v,是三个输入，其实都是输入x\n",
    "\n",
    "        d_k, d_v, nhead = self.d_k, self.d_v, self.nhead\n",
    "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "        # 输入batchsize，输入单词个数\n",
    "        residual = q\n",
    "\n",
    "        # Pass through the pre-attention projection: b x lq x (n*dv)\n",
    "        # b: batch_size, lq: translation task的seq长度, n: head数, dv: embedding vector length\n",
    "        # Separate different heads: b x lq x n x dv.\n",
    "        q = self.w_qs(q).view(sz_b, len_q, nhead, d_k)  # project & reshape\n",
    "        k = self.w_ks(k).view(sz_b, len_k, nhead, d_k)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, nhead, d_v)\n",
    "\n",
    "        # Transpose for attention dot product: bs x nhead x len_q x d_k 分成多头的输出\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "        # 先变到多头的Q K V，然后进行缩放点击注意力\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)   # For head axis broadcasting.\n",
    "            # (batchSize, 1, seqLen) -> (batchSize, 1, 1, seqLen)\n",
    "\n",
    "        q, attn = self.attention(q, k, v, mask=mask)  # 缩放点积注意力\n",
    "        # 此时的q还是多头的q\n",
    "        # Transpose to move the head dimension back: b x lq x n x d_k\n",
    "        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n",
    "        # view只能用在contiguous的variable上\n",
    "        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
    "        # contiguous()强制拷贝一份tensor，与原tensor断开联系\n",
    "        q = self.dropout(self.fc(q))  # 把多头的输出经过一个线性层换成单头维度的！！！\n",
    "        # add & norm\n",
    "        q += residual  # 添加残联接\n",
    "\n",
    "        q = self.layer_norm(q)  # 层归一化\n",
    "\n",
    "        return q, attn\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):  # 缩放点积注意力机制\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q x k^T\n",
    "        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n",
    "        # self.temperature dk^(1/2)\n",
    "\n",
    "        if mask is not None:\n",
    "            # 把mask中为0的数置为-1e9, 用于decoder中的masked self-attention\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # dim=-1表示对最后一维softmax\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))  # softmax\n",
    "        output = torch.matmul(attn, v)  # 和V矩阵的矩阵乘法\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    ''' A two-feed-forward-layer module '''\n",
    "\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_in, d_hid)  # position-wise nn.Linear(输入维度，输出维度)\n",
    "        self.w_2 = nn.Linear(d_hid, d_in)  # position-wise 自动创建权重和偏置，被自动更新\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        x = self.w_2(F.relu(self.w_1(x)))  # feed Forward经过两个线性层\n",
    "        x = self.dropout(x)  # 以及dropout层\n",
    "        # add & norm\n",
    "        x += residual\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):  # 一个完整的编码器模块\n",
    "    ''' Compose with two layers '''\n",
    "\n",
    "    def __init__(self, d_model, dim_feedforward, nhead, d_k, d_v, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        # MultiHeadAttention创建类的实例\n",
    "        self.slf_attn = MultiHeadAttention(\n",
    "            nhead, d_model, d_k, d_v, dropout=dropout)\n",
    "        # FeedForward前传\n",
    "        self.pos_ffn = PositionwiseFeedForward(\n",
    "            d_model, dim_feedforward, dropout=dropout)\n",
    "\n",
    "    def forward(self, enc_input, slf_attn_mask=None):\n",
    "        # enc_slf_attn多头自注意力机制的输出\n",
    "        # 输入enc_input分成三份，分别与wq,wk,wv相乘得到QKV矩阵\n",
    "        enc_output, enc_slf_attn = self.slf_attn(\n",
    "            enc_input, enc_input, enc_input, mask=slf_attn_mask)\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        return enc_output, enc_slf_attn\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    ''' Compose with three layers '''\n",
    "\n",
    "    def __init__(self, d_model, dim_feedforward, nhead, d_k, d_v, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # masked self-attention\n",
    "        self.slf_attn = MultiHeadAttention(\n",
    "            nhead, d_model, d_k, d_v, dropout=dropout)\n",
    "        # encoder-decoder attention\n",
    "        self.enc_attn = MultiHeadAttention(\n",
    "            nhead, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(\n",
    "            d_model, dim_feedforward, dropout=dropout)\n",
    "\n",
    "    def forward(self, dec_input, enc_output, slf_attn_mask=None, dec_enc_attn_mask=None):\n",
    "        dec_output, dec_slf_attn = self.slf_attn(  # 一个多头，输入是上一个解码器的输出\n",
    "            dec_input, dec_input, dec_input, mask=slf_attn_mask)\n",
    "        # q用自己的, k和v是encoder的输出\n",
    "        dec_output, dec_enc_attn = self.enc_attn(  # 一个多头，输入有两个是编码器的输出！！！\n",
    "            dec_output, enc_output, enc_output, mask=dec_enc_attn_mask)\n",
    "        dec_output = self.pos_ffn(dec_output)  # FeedForward层\n",
    "        return dec_output, dec_slf_attn, dec_enc_attn\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):  # 位置编码，用在输入层，每一层的开始，编码器，解码器开始\n",
    "\n",
    "    def __init__(self, d_hid, n_position=200):\n",
    "        # d_hid是维度，即每个单词由长度为多少的向量进行表示。\n",
    "        # n_position为最大位置，即所能处理的单词在句子中的最大的位置\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # 将tensor注册成buffer, optim.step()的时候不会更新\n",
    "        # 定义一组参数，参数名称为‘pos_table’，\n",
    "        # 这组tensor形式的参数为self._get_sinusoid_encoding_table(n_position, d_hid)\n",
    "        self.register_buffer(\n",
    "            'pos_table', self._get_sinusoid_encoding_table(n_position, d_hid))\n",
    "\n",
    "    def _get_sinusoid_encoding_table(self, n_position, d_hid):\n",
    "        # TODO: make it with torch instead of numpy   todo是将要做的事，用在团队协作中，好处是和注释分开\n",
    "        def get_position_angle_vec(position):\n",
    "            # 2 * (hid_j // 2) 依次为：0 0 2 2 4 4 6 6 8 8 ... 然后/d_hid归一化到0~1，再变到指数\n",
    "            # 变到指数范围在1~10000,然后用pos除以这个指数，得到范围0~200/10000，最后偶维度用sin 奇维度用cos\n",
    "            return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n",
    "\n",
    "        # 创建一个位置的array，每一行是位置i的位置嵌入向量\n",
    "        # 列表列数为词嵌入维度，列表行数为单词个数\n",
    "        sinusoid_table = np.array([get_position_angle_vec(pos_i)\n",
    "                                  for pos_i in range(n_position)])\n",
    "\n",
    "        # 返回每个位置的sin/cos括号里面的\n",
    "        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i 偶数\n",
    "        sinusoid_table[:, 1::2] = np.cos(\n",
    "            sinusoid_table[:, 1::2])  # dim 2i+1 奇数\n",
    "        # 转为Float类型的张量  # unsqueeze(0)是升维，在0维度升一维\n",
    "        return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):  # [[1,2,3], [4,5,6]] 0维是2，1维是3，越高就是越靠里面的\n",
    "        # clone()创建一个相同的tensor，不共享内存地址（数据无关），但是新tensor的梯度会叠加在源tensor上\n",
    "        # detach()是返回一个共享内存（数值一样），但是不计算grad的tensor\n",
    "        # x应该是已经经过词编码的向量 [:, :词嵌入维度d_hid]\n",
    "        return x + self.pos_table[:, :x.size(1)].clone().detach()  # 数据、梯度均无关\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):  # 编码器部分\n",
    "    ''' A encoder model with self attention mechanism. '''\n",
    "\n",
    "    def __init__(\n",
    "            self, n_src_vocab, d_word_vec, n_layers, nhead, d_k, d_v,\n",
    "            d_model, dim_feedforward, pad_idx, dropout=0.1, n_position=200):\n",
    "        # d_word_vec是单词嵌入的向量的维度，n_position是最大的单词位置\n",
    "        # n_src_vocab是源词汇表大小，pad_idx是指定单词索引为多少的表示填充\n",
    "        super().__init__()\n",
    "        # 这一步实际上是创建一个权重矩阵，(n_src_vocab, d_word_vec)维度的\n",
    "        self.src_word_emb = nn.Embedding(\n",
    "            n_src_vocab, d_word_vec, padding_idx=pad_idx)\n",
    "        # 位置编码，对每个位置进行编码\n",
    "        self.position_enc = PositionalEncoding(\n",
    "            d_word_vec, n_position=n_position)\n",
    "        self.dropout = nn.Dropout(p=dropout)  # 随机失活，将张量某些元素置为0，减少过拟合\n",
    "        # 多个Encoder Layer叠加\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            EncoderLayer(d_model, dim_feedforward, nhead,\n",
    "                         d_k, d_v, dropout=dropout)  # 多头和前传\n",
    "            for _ in range(n_layers)])  # n_layers是编码器模块个数\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "    def forward(self, src_seq, src_mask, return_attns=False):\n",
    "\n",
    "        enc_slf_attn_list = []\n",
    "\n",
    "        # -- Forward\n",
    "\n",
    "        # Embedding & Position encoding 词嵌入和位置编码，第一个编码层之前的部分\n",
    "        enc_output = self.dropout(\n",
    "            self.position_enc(self.src_word_emb(src_seq)))\n",
    "\n",
    "        # ==================================================================\n",
    "        print('encoder embedding')\n",
    "        print(self.src_word_emb(src_seq))\n",
    "        # ==================================================================\n",
    "\n",
    "        # 先进行单词索引embedding，可以认为是把每个单词索引，用一个向量替换。\n",
    "        # 再进行位置编码叠加起来，最后进行dropout\n",
    "        enc_output = self.layer_norm(enc_output)  # 然后进行LayerNorm\n",
    "\n",
    "        # Encoder Layers\n",
    "        for enc_layer in self.layer_stack:  # 然后经过编码器模块\n",
    "            enc_output, enc_slf_attn = enc_layer(\n",
    "                enc_output, slf_attn_mask=src_mask)\n",
    "            enc_slf_attn_list += [enc_slf_attn] if return_attns else []\n",
    "\n",
    "        if return_attns:\n",
    "            return enc_output, enc_slf_attn_list\n",
    "        return enc_output,\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):  # 解码器部分\n",
    "    ''' A decoder model with self attention mechanism. '''\n",
    "\n",
    "    def __init__(\n",
    "            self, n_trg_vocab, d_word_vec, n_layers, nhead, d_k, d_v,\n",
    "            d_model, dim_feedforward, pad_idx, n_position=200, dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "        # ebedding词集变成目标语言词集\n",
    "        self.trg_word_emb = nn.Embedding(\n",
    "            n_trg_vocab, d_word_vec, padding_idx=pad_idx)\n",
    "        # Position encoding\n",
    "        self.position_enc = PositionalEncoding(\n",
    "            d_word_vec, n_position=n_position)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # 多个Decoder Layer叠加\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            DecoderLayer(d_model, dim_feedforward, nhead,\n",
    "                         d_k, d_v, dropout=dropout)\n",
    "            for _ in range(n_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "    def forward(self, trg_seq, trg_mask, enc_output, src_mask, return_attns=False):\n",
    "\n",
    "        dec_slf_attn_list, dec_enc_attn_list = [], []\n",
    "\n",
    "        # -- Forward\n",
    "\n",
    "        # Embedding & Position encoding\n",
    "        dec_output = self.dropout(\n",
    "            self.position_enc(self.trg_word_emb(trg_seq)))\n",
    "\n",
    "        # ============================================================\n",
    "        print('decoder embedding:')\n",
    "        print(self.trg_word_emb(trg_seq))\n",
    "        # ============================================================\n",
    "\n",
    "        # 先对目的句子进行词嵌入以及位置编码，然后dropout，然后LN层\n",
    "        dec_output = self.layer_norm(dec_output)\n",
    "\n",
    "        # Decoder Layers\n",
    "        for dec_layer in self.layer_stack:\n",
    "            dec_output, dec_slf_attn, dec_enc_attn = dec_layer(  # 每一层输入都有编码器输出的\n",
    "                dec_output, enc_output, slf_attn_mask=trg_mask, dec_enc_attn_mask=src_mask)\n",
    "            dec_slf_attn_list += [dec_slf_attn] if return_attns else []\n",
    "            dec_enc_attn_list += [dec_enc_attn] if return_attns else []\n",
    "\n",
    "        if return_attns:\n",
    "            return dec_output, dec_slf_attn_list, dec_enc_attn_list\n",
    "        return dec_output,\n",
    "\n",
    "\n",
    "# def get_pad_mask(seq, pad_idx):\n",
    "#     # (batch, seqlen) -> (batch, 1, seqlen)\n",
    "#     # pad_idx是填充的索引值\n",
    "#     # 输出是(batch, 1, seqlen)，填充位置被标记为false\n",
    "#     return (torch.tensor(seq != pad_idx)).unsqueeze(-2)  # (batch, 1, seqlen)\n",
    "\n",
    "\n",
    "# def get_subsequent_mask(seq):\n",
    "#     ''' For masking out the subsequent info. '''\n",
    "#     sz_b, len_s = seq.size()\n",
    "#     # torch.triu(diagonal=1)保留矩阵上三角部分，其余部分(包括对角线)定义为0。\n",
    "#     subsequent_mask = (1 - torch.triu(\n",
    "#         torch.ones((1, len_s, len_s), device=seq.device), diagonal=1)).bool()\n",
    "#     return subsequent_mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # self.src_pad_idx, self.trg_pad_idx = src_pad_idx, trg_pad_idx\n",
    "        # # mask\n",
    "        # src_mask = get_pad_mask(src_seq, self.src_pad_idx)\n",
    "        # trg_mask = get_pad_mask(\n",
    "        #     trg_seq, self.trg_pad_idx) & get_subsequent_mask(trg_seq)\n",
    "# # Encoder 创建类的实例\n",
    "#         self.encoder = Encoder(\n",
    "#             n_src_vocab=n_src_vocab, n_position=n_position,\n",
    "#             d_word_vec=d_word_vec, d_model=d_model, dim_feedforward=dim_feedforward,\n",
    "#             n_layers=num_encoder_layers, nhead=nhead, d_k=d_k, d_v=d_v,\n",
    "#             pad_idx=src_pad_idx, dropout=dropout)\n",
    "\n",
    "#         # Decoder 创建类的实例\n",
    "#         self.decoder = Decoder(\n",
    "#             n_trg_vocab=n_trg_vocab, n_position=n_position,\n",
    "#             d_word_vec=d_word_vec, d_model=d_model, dim_feedforward=dim_feedforward,\n",
    "#             n_layers=num_decoder_layers, nhead=nhead, d_k=d_k, d_v=d_v,\n",
    "#             pad_idx=trg_pad_idx, dropout=dropout)\n",
    "\n",
    "#         # 最后的linear输出层\n",
    "#         self.trg_word_prj = nn.Linear(d_model, n_trg_vocab, bias=False)\n",
    "\n",
    "\n",
    "                #  n_src_vocab, n_trg_vocab, src_pad_idx, trg_pad_idx,\n",
    "                #  d_word_vec=8,\n",
    "                #  d_k=64, d_v=64, n_position=200,\n",
    "                #  trg_emb_prj_weight_sharing=False, emb_src_trg_weight_sharing=False):\n",
    "\n",
    "                \n",
    "        # n_src_vocab是源词汇索引表的大小\n",
    "        # n_trg_vocab是目的词汇索引表的大小\n",
    "        # src_pad_idx是源词汇索引表中需要补充padding对应的索引的值\n",
    "        # trg_pad_idx是目的词汇索引表中需要补充padding对应的索引的值\n",
    "\n",
    "        # d_word_vec是单词向量的维度\n",
    "        # n_layers是有几个编码层，有几个解码层\n",
    "\n",
    "        # d_k nhead*d_k是多头自注意力里面矩阵乘法输出的维度\n",
    "        # d_v nhead*d_v是多头自注意力里面矩阵乘法输出的维度\n",
    "\n",
    "        # n_position是最大的单词位置\n",
    "\n",
    "        # trg_emb_prj_weight_sharing是否共享权重\n",
    "        # emb_src_trg_weight_sharing是否共享权重\n",
    "\n",
    "        \n",
    "        # self.x_logit_scale = 1.\n",
    "        # if trg_emb_prj_weight_sharing:\n",
    "        #     # Share the weight between target word embedding & last dense layer\n",
    "        #     self.trg_word_prj.weight = self.decoder.trg_word_emb.weight\n",
    "        #     self.x_logit_scale = (d_model ** -0.5)\n",
    "\n",
    "        # if emb_src_trg_weight_sharing:\n",
    "        #     self.encoder.src_word_emb.weight = self.decoder.trg_word_emb.weight\n",
    "\n",
    "                # enc_output, *_ = self.encoder(src_seq, src_mask)\n",
    "        # dec_output, *_ = self.decoder(trg_seq, trg_mask, enc_output, src_mask)\n",
    "\n",
    "        # # ==========================================================\n",
    "        # print('dec_output:')\n",
    "        # print(dec_output.shape)\n",
    "        # print(dec_output)\n",
    "        # # ==========================================================\n",
    "\n",
    "        # # final linear layer得到logit vector最后的一个线性层，输出通道为输出词汇表大小\n",
    "        # seq_logit = self.trg_word_prj(\n",
    "        #     dec_output) * self.x_logit_scale  # x_logit_scale是缩放\n",
    "        # # print(seq_logit.shape)\n",
    "        # seq_logit = F.softmax(seq_logit, dim=2)  # 最后一个softmax\n",
    "        # # 应该最后再加一个softmax\n",
    "        # return seq_logit.view(-1, seq_logit.size(2))\n",
    "\n",
    "                # src_seq输入序列张量,(batchsize,seqlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
    "        #     query, key, value, self.embed_dim, self.num_heads,\n",
    "        #     self.in_proj_weight, self.in_proj_bias,\n",
    "        #     self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "        #     self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "        #     training=False,\n",
    "        #     key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "        #     attn_mask=attn_mask,\n",
    "        #     average_attn_weights=average_attn_weights,\n",
    "        #     is_causal=is_causal)\n",
    "        # # print('attn_output:')\n",
    "        # # print(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# W_q = nn.Linear(512, 2048, bias=True)\n",
    "# x = torch.rand((10,512))\n",
    "# print(W_q.weight.shape)\n",
    "# print(x.shape)\n",
    "# print(W_q(x).shape)\n",
    "# K = torch.tensor([[1,2,3,0,0],[1,5,4,6,0]])\n",
    "# key_masks = torch.sign(torch.abs(K))\n",
    "# print(key_masks)\n",
    "# key_masks = torch.unsqueeze(key_masks, 1)\n",
    "# print(key_masks)\n",
    "# key_masks = key_masks.repeat(1, 5, 1)\n",
    "# print(key_masks)\n",
    "# diag_vals = torch.ones((3,3))\n",
    "\n",
    "# tril = torch.tril(diag_vals, diagonal=0)\n",
    "# print(tril)\n",
    "\n",
    "label = torch.tensor([[1,2,5,8],[1,5,3,4]])\n",
    "y_onehot = torch.zeros(2 * 4, 10)\n",
    "y_onehot = y_onehot.scatter_(1,label.view(-1,1).data,1)\n",
    "print(y_onehot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
